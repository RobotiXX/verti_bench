{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torch import nn, Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from torch import distributions as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For one dataset\n",
    "class BMPLoader(Dataset):\n",
    "    def __init__(self, base_directory):\n",
    "        self.base_directory = base_directory\n",
    "        self.file_names = [os.path.join(base_directory, f) for f in os.listdir(base_directory) if f.endswith('.bmp')]\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),  # Converts to [0, 1] range\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1] range\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.file_names[idx])\n",
    "        img = img.convert('L')  # Convert to grayscale\n",
    "        return self.transform(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min/Max for Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the dataset: 20000\n"
     ]
    }
   ],
   "source": [
    "# Data for Benchmark\n",
    "dataset = BMPLoader('../data/terrain_bitmaps/BenchMaps/sampled_maps/TrainSet')\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Print the number of images in the dataset\n",
    "print(f\"Number of images in the dataset: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from abc import abstractmethod\n",
    "from typing import Callable, List, Any, Optional, Sequence, Type\n",
    "\n",
    "class BaseVAE(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super(BaseVAE, self).__init__()\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, input: Tensor) -> Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, batch_size:int, current_device: int, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *inputs: Tensor) -> Tensor:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss_function(self, *inputs: Any, **kwargs) -> Tensor:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWAE(BaseVAE):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims: List = None,\n",
    "                 reg_weight: int = 100,\n",
    "                 wasserstein_deg: float= 2.,\n",
    "                 num_projections: int = 200,\n",
    "                 projection_dist: str = 'normal',\n",
    "                    **kwargs) -> None:\n",
    "        super(SWAE, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.reg_weight = reg_weight\n",
    "        self.p = wasserstein_deg\n",
    "        self.num_projections = num_projections\n",
    "        self.proj_dist = projection_dist\n",
    "\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size = 3, stride = 2, padding = 1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_z = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1]*4)\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride = 2,\n",
    "                                       padding=1,\n",
    "                                       output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                               hidden_dims[-1],\n",
    "                                               kernel_size=3,\n",
    "                                               stride=2,\n",
    "                                               padding=1,\n",
    "                                               output_padding=1),\n",
    "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1], out_channels= 1,\n",
    "                                      kernel_size= 3, padding= 1),\n",
    "                            nn.Tanh())\n",
    "\n",
    "    def encode(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        z = self.fc_z(result)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, 512, 2, 2)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n",
    "        z = self.encode(input)\n",
    "        return  [self.decode(z), input, z]\n",
    "\n",
    "    def loss_function(self, recons, input, z) -> dict:\n",
    "        batch_size = input.size(0)\n",
    "        bias_corr = batch_size *  (batch_size - 1)\n",
    "        reg_weight = self.reg_weight / bias_corr\n",
    "\n",
    "        recons_loss_l2 = F.mse_loss(recons, input)\n",
    "        recons_loss_l1 = F.l1_loss(recons, input)\n",
    "\n",
    "        swd_loss = self.compute_swd(z, self.p, reg_weight)\n",
    "\n",
    "        loss = recons_loss_l2 + recons_loss_l1 + swd_loss\n",
    "        return loss\n",
    "\n",
    "    def get_random_projections(self, latent_dim: int, num_samples: int) -> Tensor:\n",
    "        \"\"\"\n",
    "        Returns random samples from latent distribution's (Gaussian)\n",
    "        unit sphere for projecting the encoded samples and the\n",
    "        distribution samples.\n",
    "\n",
    "        :param latent_dim: (Int) Dimensionality of the latent space (D)\n",
    "        :param num_samples: (Int) Number of samples required (S)\n",
    "        :return: Random projections from the latent unit sphere\n",
    "        \"\"\"\n",
    "        if self.proj_dist == 'normal':\n",
    "            rand_samples = torch.randn(num_samples, latent_dim)\n",
    "        elif self.proj_dist == 'cauchy':\n",
    "            rand_samples = dist.Cauchy(torch.tensor([0.0]),\n",
    "                                       torch.tensor([1.0])).sample((num_samples, latent_dim)).squeeze()\n",
    "        else:\n",
    "            raise ValueError('Unknown projection distribution.')\n",
    "\n",
    "        rand_proj = rand_samples / rand_samples.norm(dim=1).view(-1,1)\n",
    "        return rand_proj # [S x D]\n",
    "\n",
    "\n",
    "    def compute_swd(self,\n",
    "                    z: Tensor,\n",
    "                    p: float,\n",
    "                    reg_weight: float) -> Tensor:\n",
    "        \"\"\"\n",
    "        Computes the Sliced Wasserstein Distance (SWD) - which consists of\n",
    "        randomly projecting the encoded and prior vectors and computing\n",
    "        their Wasserstein distance along those projections.\n",
    "\n",
    "        :param z: Latent samples # [N  x D]\n",
    "        :param p: Value for the p^th Wasserstein distance\n",
    "        :param reg_weight:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        prior_z = torch.randn_like(z) # [N x D]\n",
    "        device = z.device\n",
    "\n",
    "        proj_matrix = self.get_random_projections(self.latent_dim,\n",
    "                                                  num_samples=self.num_projections).transpose(0,1).to(device)\n",
    "\n",
    "        latent_projections = z.matmul(proj_matrix) # [N x S]\n",
    "        prior_projections = prior_z.matmul(proj_matrix) # [N x S]\n",
    "\n",
    "        # The Wasserstein distance is computed by sorting the two projections\n",
    "        # across the batches and computing their element-wise l2 distance\n",
    "        w_dist = torch.sort(latent_projections.t(), dim=1)[0] - \\\n",
    "                 torch.sort(prior_projections.t(), dim=1)[0]\n",
    "        w_dist = w_dist.pow(p)\n",
    "        return reg_weight * w_dist.mean()\n",
    "    \n",
    "    def freeze_encoder(self):\n",
    "        for param in list(self.encoder.parameters()) + list(self.fc_z.parameters()):\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min values for Bench:  [-3.7062101 -3.8909993 -3.7361746 -3.9380963 -3.3592634 -3.4875672\n",
      " -3.8780265 -3.4319181 -3.9965646 -4.0892687 -3.8394182 -3.5107944\n",
      " -3.6686323 -3.9672284 -3.6471565 -3.422289  -3.8330948 -4.462198\n",
      " -3.6393564 -3.7785573 -3.674911  -4.2584405 -4.0545974 -3.8244252\n",
      " -3.7042878 -3.4703813 -3.7105165 -3.8041348 -4.7422585 -3.783793\n",
      " -4.1101017 -3.9899917 -3.9294493 -3.7566257 -4.047484  -3.8708837\n",
      " -3.99885   -3.6657593 -3.7299051 -3.5601566 -3.5462823 -3.5642083\n",
      " -3.4795258 -3.5732584 -3.8238697 -4.074727  -4.130914  -4.7083416\n",
      " -3.803444  -3.5453184 -3.8299093 -3.4622931 -4.421817  -3.7873077\n",
      " -3.6150494 -3.9056745 -4.031999  -4.235405  -4.1596203 -3.757993\n",
      " -4.301626  -4.3440065 -3.5704322 -3.993898 ]\n",
      "Max values for Bench:  [3.47078   3.6544452 3.6182761 3.7519124 3.6562674 4.493456  4.391973\n",
      " 3.4939077 3.5075793 3.500696  3.706252  3.616844  3.761067  3.731271\n",
      " 3.8254223 3.8154802 3.7171452 4.0293217 4.2769675 3.8128085 3.706255\n",
      " 4.016803  3.5142694 3.8107374 3.4549801 3.939258  4.763484  3.46614\n",
      " 4.7694664 4.04117   3.964012  3.953903  3.6537955 3.5488381 4.052981\n",
      " 4.4212556 3.3692474 3.9716275 3.441842  3.6963007 3.9237947 3.8596387\n",
      " 3.6765776 3.7377908 4.4320073 3.793932  4.2403502 4.1358433 3.7805364\n",
      " 3.9487348 3.7901335 3.6302912 3.3391788 4.199501  3.888131  3.7451353\n",
      " 4.2167115 3.2608192 3.959318  3.6398258 3.6453447 3.738882  4.017051\n",
      " 3.7511394]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the models\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the trained model and set it to evaluation mode\n",
    "swae_model = SWAE(in_channels=1, latent_dim=64)\n",
    "swae_model.load_state_dict(torch.load('./BenchElev.pth', map_location=device, weights_only=True))\n",
    "swae_model.eval()\n",
    "\n",
    "# Move models to the appropriate device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "swae_model.to(device)\n",
    "\n",
    "# Initialize min and max vectors\n",
    "min_vector = torch.full((64,), float('inf')).to(device)\n",
    "max_vector = torch.full((64,), float('-inf')).to(device)\n",
    "\n",
    "# Iterate through the dataset to find the min and max values\n",
    "with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(device)\n",
    "        latent_vector = swae_model.encode(batch)\n",
    "        \n",
    "        # Update min and max vectors\n",
    "        min_vector = torch.min(min_vector, latent_vector.min(dim=0)[0])\n",
    "        max_vector = torch.max(max_vector, latent_vector.max(dim=0)[0])\n",
    "\n",
    "# Convert min and max values to numpy arrays if needed\n",
    "min_vectorACL = min_vector.cpu().numpy()\n",
    "max_vectorACL = max_vector.cpu().numpy()\n",
    "\n",
    "np.save('min_vectorBench.npy', min_vectorACL)\n",
    "np.save('max_vectorBench.npy', max_vectorACL)\n",
    "\n",
    "print(\"Min values for Bench: \", min_vectorACL)\n",
    "print(\"Max values for Bench: \", max_vectorACL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min/Max for MCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the dataset: 50000\n"
     ]
    }
   ],
   "source": [
    "# Data for MCL\n",
    "dataset = BMPLoader('../data/terrain_bitmaps/Manual-CL/TrainSet')\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Print the number of images in the dataset\n",
    "print(f\"Number of images in the dataset: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min values for MCL:  [-4.026846  -3.7844894 -3.113536  -3.2230625 -4.7366796 -4.057123\n",
      " -3.706329  -3.392455  -3.8743103 -3.8935816 -3.4339774 -3.3696833\n",
      " -4.183838  -3.5761437 -3.6986923 -3.6437447 -3.6003337 -3.6840518\n",
      " -4.4190817 -3.5381734 -4.2054825 -3.8751025 -4.0558653 -4.286704\n",
      " -4.062463  -3.2248898 -3.6386237 -3.290187  -3.7026496 -3.8115523\n",
      " -2.063816  -4.2707257 -3.9774156 -4.296227  -3.4535143 -3.6449165\n",
      " -3.4875813 -3.6147952 -3.4858165 -3.4618804 -3.7035217 -3.3803794\n",
      " -4.1484838 -3.5151718 -3.7985313 -3.2333407 -3.2580166 -3.656097\n",
      " -3.101282  -3.7456577 -3.6446877 -3.7860909 -3.8346455 -3.3757591\n",
      " -3.464241  -3.337626  -4.181528  -3.6549032 -3.5443556 -2.9420679\n",
      " -3.2472672 -3.2864485 -3.2318344 -3.0205083]\n",
      "Max values for MCL:  [4.122861  3.4537418 3.812586  3.7506456 3.3736398 3.5027428 2.5564022\n",
      " 3.6559896 3.690561  4.2581143 3.1773424 3.2667673 3.7388036 3.7841072\n",
      " 4.3424907 3.53028   3.6651552 3.109188  3.8801513 3.5555248 3.111924\n",
      " 3.1390214 3.2859235 2.8316622 4.0872216 3.4550033 3.8614864 3.4669125\n",
      " 3.3496995 3.5933526 3.1136541 3.8026695 3.6179526 3.4073627 3.5802383\n",
      " 3.339991  2.8730454 3.19965   3.4214118 3.8367283 4.221074  3.614158\n",
      " 3.101617  3.6108882 3.4515772 3.8607886 3.6674871 3.730194  3.2828217\n",
      " 3.4065106 4.1191716 3.3003943 3.4628124 3.626906  4.1273603 3.5698085\n",
      " 3.2549539 3.351132  3.8389466 4.0818925 4.3547726 3.9459927 4.275451\n",
      " 4.360732 ]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the models\n",
    "swae_model = SWAE(in_channels=1, latent_dim=64)\n",
    "swae_model.load_state_dict(torch.load('./SWAE_64.pth'))\n",
    "swae_model.eval()\n",
    "\n",
    "# Move models to the appropriate device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "swae_model.to(device)\n",
    "\n",
    "# Initialize min and max vectors\n",
    "min_vector = torch.full((64,), float('inf')).to(device)\n",
    "max_vector = torch.full((64,), float('-inf')).to(device)\n",
    "\n",
    "# Iterate through the dataset to find the min and max values\n",
    "with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(device)\n",
    "        latent_vector = swae_model.encode(batch)\n",
    "        \n",
    "        # Update min and max vectors\n",
    "        min_vector = torch.min(min_vector, latent_vector.min(dim=0)[0])\n",
    "        max_vector = torch.max(max_vector, latent_vector.max(dim=0)[0])\n",
    "\n",
    "# Convert min and max values to numpy arrays if needed\n",
    "min_vectorMCL = min_vector.cpu().numpy()\n",
    "max_vectorMCL = max_vector.cpu().numpy()\n",
    "\n",
    "np.save('min_vectorMCL.npy', min_vectorMCL)\n",
    "np.save('max_vectorMCL.npy', max_vectorMCL)\n",
    "\n",
    "print(\"Min values for MCL: \", min_vectorMCL)\n",
    "print(\"Max values for MCL: \", max_vectorMCL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chrono",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
